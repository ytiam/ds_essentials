{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### s3fs ###\n",
    "def test_s3fs(bucket,file):\n",
    "    try:\n",
    "        import s3fs\n",
    "        fs = s3fs.S3FileSystem(anon=False)\n",
    "        fs.ls('%s'%(bucket))\n",
    "        with fs.open('%s/%s'%(bucket,file), 'rb') as f:\n",
    "            print(f.read())\n",
    "        print('s3fs OK')\n",
    "    except Exception as e:\n",
    "        print('s3fs:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'index,SYMB_SFX_CD\\nabi-1,81103\\npdl-1,63047\\ncol-0,54892\\nabi-2,53006\\nCOL-0,50995\\nrrr-0,48628\\nPDL-1,44203\\nNBM-1,36039\\nRRR-0,34777\\nNBM-0,34637\\nnbm-1,33972\\nnbm-0,33909\\nabi-3,33325\\ncwl-0,31597\\nABI-1,27764\\nnbm-2,24089\\nrtt-0,23976\\nNBM-2,23203\\nabi-4,18551\\nABI-2,18134\\npdo-1,17540\\ncww-0,17509\\npdl-2,16117\\nPDL-2,14104\\nABO-1,14014\\nPOLICY,13677\\nnbm-3,13555\\nNBM-3,11973\\nncl-0,11262\\nABO-2,11103\\npdi-1,10384\\nABI-3,10271\\nasa-1,10093\\nAUO-0,9854\\nAUO-1,9638\\naub-0,9494\\nabi-5,9361\\namp-1,9343\\namp-0,8897\\nABO-3,8206\\nrvm-0,8201\\nasa-2,8082\\nauo-0,7996\\nNBE-0,7838\\ncil-0,7556\\nAUO-2,7418\\npolicy,7301\\nnbm-4,7112\\nCOl-0,6974\\naub-1,6836\\nauo-1,6747\\nnbe-0,6713\\nNBE-1,6475\\nPDI-1,6403\\nPDO-1,6348\\nANO-1,6175\\namp-2,6165\\nANO-0,6009\\naub-2,5911\\npdl-3,5817\\nPDL,5668\\nNBM-4,5539\\nABI-4,5513\\nPDL-3,5480\\nasa-3,5448\\nRTT-0,5145\\nauo-2,4962\\npdo-2,4785\\nabo-1,4775\\naup-0,4728\\nANO-2,4542\\nabo-2,4485\\nABO-4,4355\\nabi-6,4313\\nnbe-1,3739\\nAMP-0,3689\\nnbm-5,3640\\nCIL-0,3565\\nCOL,3507\\nAUO-3,3420\\nasa-4,3220\\nABI-5,3167\\nPDL-4,3020\\naub-3,2973\\namp-3,2836\\npdl-4,2829\\nAMP-1,2820\\nNBS-1,2801\\nabo-3,2736\\nauo-3,2723\\nAUB-0,2713\\nCWL-0,2700\\nPDI-2,2699\\npdi-2,2688\\nrao-0,2659\\nAUU-0,2621\\nABO-5,2617\\nNBE-2,2560\\nCWW-0,2430\\nauu-0,2357\\nNBS-0,2340\\nPDO-2,2288\\nNBM-5,2187\\nabi-7,2172\\nAUU-1,2126\\nrfi-0,2122\\nANO-3,2116\\nAUB-1,2050\\nnbe-2,1975\\nraa-0,1932\\naub-4,1855\\nAMP-2,1837\\npdl,1826\\nano-0,1775\\nauu-1,1741\\ncol,1634\\nRVM-0,1604\\npolcy,1590\\nAUU-2,1573\\nabo-4,1559\\nABI-6,1557\\nabi-0,1554\\nrpt-0,1540\\nAUO-4,1533\\nrah-0,1517\\npdo-3,1514\\nnbm-6,1482\\nAUP-0,1469\\namp-4,1455\\nAUB-2,1453\\nprem+-1,1440\\nauo-4,1376\\nasa-5,1368\\nPDI,1356\\nano-1,1343\\nnbe-3,1336\\nABO-6,1309\\npdl-5,1182\\nPDL-5,1161\\nCWL,1146\\nabi-8,1075\\nNBE-3,1059\\nRAO-0,1026\\nNBS-2,1026\\naub-5,980\\nano-2,962\\nPDI-3,957\\nabi,945\\nCWW,932\\nNBM-6,924\\npdi-3,923\\nABI-0,887\\nPDO-3,884\\nccp-0,884\\nabo-5,825\\nABI-7,824\\nnbm,817\\nANO-4,799\\nASSIST,796\\nauo-5,788\\nAUB-3,778\\nauu-2,766\\nnbs-0,744\\npdo-4,708\\nABI,700\\nrfl-0,692\\nAUU-3,689\\nano-3,679\\nNBS-3,678\\nASA-2,672\\nPDL-6,671\\ncob-0,667\\nAUO-5,656\\nabi-9,649\\namp,649\\nRFI-0,646\\nRAA-0,639\\nASA-1,625\\nABO-0,603\\namp-5,589\\nasa-6,584\\nAMP-3,563\\nCCP-0,538\\nrpe-0,528\\nrgl-0,524\\nnbe-4,519\\nnbs-1,510\\npdi-4,508\\nANF-1,508\\nauu,506\\nabo-0,479\\nPDI-4,468\\nASSIST-0,436\\nnbm-7,432\\nAUO-6,424\\nabo-6,420\\nANO-5,416\\nAUU-4,412\\nABO-7,410\\nNBE-4,403\\nANF-0,402\\npdl-7,399\\nRTT,396\\npdr-1,386\\nANF-2,382\\nNCL-0,378\\npdl1,378\\npdl-6,374\\nrtt,374\\nauu-5,360\\nabi1,357\\nABO-8,352\\nANO,350\\nauo-6,349\\nABI-8,346\\nrwn-0,340\\nRGL-0,333\\nPDO-4,326\\nNBM,325\\nRPT-0,324\\naub-6,320\\nasc-2,314\\nasc-1,313\\nrrr0,306\\nabi-a,304\\nabi-b,301\\nABI-9,299\\n - ,295\\nauu-4,295\\nanf-1,294\\nRRR,294\\ncdv-0,294\\nPDO,293\\nAUB,292\\nASA-3,290\\nNBM-7,288\\nabi2,286\\nnbs-2,284\\nCIL,279\\nnem-0,276\\nrrr,275\\nAUB-4,273\\nAMP-4,272\\npdl-0,271\\nRPE-0,270\\nCIl-0,261\\nauu-3,256\\nNBE,256\\nnem-1,254\\npdr-2,251\\nASA-4,251\\nAMP,246\\npdo,246\\npdi,245\\nPDI-0,244\\nano-4,243\\nabi4,237\\nAMP-5,236\\nNBM-8,235\\ncol0,235\\nRWN-0,234\\nabi3,233\\nnbe,232\\nanf-0,232\\npdo-5,228\\namp-7,227\\nanf-2,225\\nRFL-0,223\\npdi-0,220\\npdr-5,219\\nrcp-0,218\\namp-6,212\\npdo-0,210\\nanf-3,208\\nNES-1,200\\nPDO-5,199\\nauo-7,199\\npdl-8,195\\nprem+,194\\nPDl-1,193\\nasa-7,192\\nNEM-1,190\\nPDL-0,188\\naub,187\\ncrc-0,182\\nAMP-6,181\\nPDL-7,169\\nnbm2,167\\nnbm0,165\\nANF-3,160\\nNBS-4,159\\nABI-A,154\\ncww-o,148\\nPDR-1,143\\nPDL-9,142\\nrvm,142\\nRVM,142\\nNEE-0,142\\nnbs,142\\nNBE-5,140\\nnbs-3,138\\npdr-3,136\\ncil,136\\nrow-0,134\\nAMP-7,133\\nnbe-6,133\\nASA -1,132\\nNBM-9,131\\n ,131\\npdi-5,131\\nabi-c,131\\nnem-2,130\\nano,129\\nabi5,128\\nabi6,128\\nprem-1,128\\nPDO-6,127\\nABi-1,126\\nAUU-5,126\\nRAH-0,125\\nANO-6,123\\nasa-c,123\\npdl2,123\\nNBm-0,123\\nABO,122\\nNEE-1,122\\namo-0,121\\nasa-9,118\\nNES-0,117\\nano-5,116\\nasa-8,113\\nasa,113\\npdi2,113\\nnbe0,113\\npolcy-1,112\\nANF-4,112\\nnbe-5,111\\nNBm-2,111\\nRFI,111\\nPDI-5,110\\nPDV-1,110\\nNBm-1,109\\nprem+-0,109\\npdl-9,109\\nABi-3,108\\nAUO-7,108\\nAMP-9,108\\namp1,106\\nPDL-10,105\\nrrr-o,105\\nAUU,104\\nPDL-8,104\\nPOLCY,103\\nano-6,99\\npdo-7,98\\nAUB-6,98\\nauo,98\\nhtp-0,97\\nAMP-o,96\\nANo-0,96\\nnbs0,96\\nncl0,96\\nAUB-5,96\\nasc-3,95\\nNMB-2,95\\nPREM+,94\\nnpi-1,94\\npdl-a,93\\npolcy-0,93\\nRFI-1,93\\nano1,92\\nrtt0,91\\nNOM-1,90\\nPDR-2,88\\nnbm-8,87\\naup,86\\nCDV-0,85\\nasc-4,84\\ncww,84\\nNEM-3,84\\nNOE-0,82\\nNOM-0,82\\nnbm-a,82\\naub2,82\\namp -1,80\\nnpi-2,78\\nPDL Closed,78\\nCOB-0,77\\npdo2,75\\nNEM-4,74\\nNBS-5,74\\nNOM-4,74\\nabo-8,74\\nNOM-5,74\\npdo-6,74\\nNOE-3,74\\nNOM-3,74\\nNEM-2,74\\nNEM-5,74\\nraa,73\\nabo-b,73\\nABI-B,73\\nabo-7,73\\nabo-a,73\\naub1,72\\nnpi-3,72\\nnpi-5,72\\ncwl,71\\nPOLCY-0,71\\nNDB-0,70\\naub -3,70\\nhtj-0,70\\nAUO-8,69\\nNBE-7,68\\npdo1,68\\nABO-9,68\\naub3,67\\naup-1,67\\nRAO,66\\nncl,65\\nhmd-0,64\\nau,64\\nPDl-2,64\\nPDO-0,63\\nnbm4,62\\nano4,62\\npdi3,62\\nrao,62\\nano0,62\\nnbm-9,61\\naub -0,60\\npcl-1,60\\naub-7,59\\nCOL Closed,59\\nAUP,59\\nCIL-O,58\\npdl3,58\\nRPT,58\\nnes-2,57\\nasc-5,56\\nAUO-9,56\\nanf-4,55\\nabI-1,55\\n1,55\\nROW-0,54\\nauo-8,54\\nASA-5,54\\npdv-1,54\\nabu-1,54\\nnee-0,54\\npdm-1,53\\namp3,53\\n0,52\\nNEM-0,52\\nnbm3,52\\nANO-8,51\\nAUU-6,51\\npdi1,49\\nNBE-6,49\\nABO-11,49\\nabi7,48\\namp2,48\\nasc-6,47\\nNOE-1,46\\nAUO,46\\nhdv-0,46\\ncol-o,46\\nabo-c,45\\nasa-b,45\\npdr-0,45\\nANHO-1,44\\nABi-2,44\\nAN0-0,44\\nPDI-6,44\\nCOL-1,43\\nnbe-7,43\\nabo-9,43\\nprem-0,42\\ncob0,41\\nASA,41\\ncob,40\\npip-3,40\\namp-8,40\\nrsa-0,40\\npip-5,40\\nNBS,40\\npip-6,40\\npip-4,40\\nnee-1,40\\nasa-a,40\\npip-2,40\\npolicy-0,39\\nmmo-0,39\\nCCP,39\\nANO-7,38\\nNMB-1,38\\nabo,38\\nPDL-B,38\\nabi-C,38\\npdm-4,38\\npdi-6,37\\nabi -5,37\\nrai-0,37\\nNOM-2,37\\nAMp-1,36\\nNBM-10,36\\nnen-0,36\\nABI-10,36\\nABI-11,36\\nNBS-6,36\\nRCP-0,35\\nANI-1,35\\nRAA,35\\nANF-5,34\\ncolo,34\\npd01,34\\nnmb-2,33\\nNBE-9,33\\nNEE-9,33\\nANU-4,33\\nPDR-4,32\\nPDR-3,32\\nWL-0,32\\nNM-0,32\\ncil0,31\\nASA-7,31\\nASA-6,31\\nrfi,30\\nab1-1,30\\nnbe2,30\\nrfi-1,30\\nPDO-7,29\\nABO-12,29\\nCWW -0,29\\nNEN-2,29\\nauo2,28\\nnsm-3,28\\nPDR,28\\nAB1-2,28\\npdr-4,28\\nnss-1,28\\nnbm1,28\\nNBE-o,27\\nnmo-0,27\\nPdl-1,27\\nPD0-2,27\\nHTP-0,27\\nASC-1,26\\nmto-0,26\\nABI -1,26\\nNMB-0,26\\n\"NOM,-1\",26\\nsbi-2,25\\nAMp-0,25\\nODL-1,25\\npd0-1,25\\nabi-12,24\\nRDV-0,24\\nRFL,24\\naub-c,24\\nRSA-0,24\\nnbm-12,24\\nmmf-0,23\\ncll-0,23\\nAUP-1,23\\nABi-4,23\\nRRR-o,23\\nPDL -1,22\\npdi-7,22\\nnmb-1,22\\nPOLICY-0,22\\nCOB,22\\nAB0-3,22\\nPDL-3 ,22\\nnb-2,21\\nNPP-1,21\\nabo-o,21\\nnom-2,21\\nCol-0,21\\n RRR-0,21\\naup-2,21\\nnee-2,21\\nraa-o,21\\nhxh-0,20\\nANF-6,20\\ncww`-0,20\\nAUO -1,20\\namo,20\\nPDM-1,20\\nhcv-0,20\\nBM-10,20\\nnmb-0,19\\nabu-4,19\\npdl4,19\\npdo4,19\\nPDI-9,19\\npdo3,19\\nauu1,19\\nnmb,19\\nabi -3,19\\nASA-0,18\\nCCP -0,18\\nodl-1,18\\navo,18\\npd0-2,18\\npdm,18\\nela-2,18\\naub -2,18\\ncol-1,18\\nraa0,18\\nCOl-9,17\\nPFL-1,17\\ncrc -0,17\\n`ABI-2,17\\nAUB-A,17\\nCLL-0,17\\nNBM-A,17\\nmbm1,17\\nNBNM-2,16\\nABO-A,16\\npdl-b,16\\n3,16\\nABI -4,16\\nNEE-2,16\\nnes-0,16\\nabi -1,16\\n[pdi-1,16\\nrfi0,16\\nano-7,16\\nABO-B,16\\nHTJ-0,15\\naub4,15\\nrdv-0,15\\namp4,15\\nNDB-3,15\\namp5,15\\nNOE-2,15\\nanf-5,15\\npdkl-1,15\\naub1 ,15\\nrah,15\\npdm-0,15\\npdi-8,15\\nPDR-5,14\\nAUO -2,14\\nCWL-O,14\\n1bi-3,14\\nnpi-6,14\\nAMp-6,14\\nhtu-0,14\\nCOl,14\\nAB0-0,14\\nvbm-1,14\\nau0-2,14\\nNFN-2,13\\nco-0,13\\nABI-O,13\\naBI-2,13\\nrmv-0,13\\nayb-2,13\\nabi -2,13\\nnmb-3,13\\nauo1,13\\naup0,13\\nasa2,12\\nano2,12\\nau0-0,12\\nRSA,12\\nrfl,12\\nCOL-5,12\\nayb-0,12\\nAMO-0,12\\nAB)-1,12\\nrah0,12\\nASA-8,12\\nhlp-1,12\\n\"a,p-2\",12\\nvcl-0,12\\nRRR`-0,11\\npdi-l,11\\nfri-0,11\\nPCL-3,11\\npdm-2,11\\n4-1,11\\nhdh-0,10\\nrgl,10\\nab1-3,10\\nDPL-1,10\\nauo-o,10\\nnsm-1,10\\nRWM-0,10\\nAUP -0,10\\nrcp,10\\nRVW-0,10\\nPFL-2,10\\nrao-1,10\\nNBm-3,10\\nFRI-0,10\\nABO -1,10\\nnm0-0,10\\npdl-32,10\\nnmo-2,10\\nAOU-1,10\\nPDL2,10\\naub -1,9\\nncl-1,9\\n`COL-0,9\\nRAA -0,9\\nABI1,9\\ncw;-0,9\\nMMF-0,9\\nABF-1,9\\nPOLCY+,9\\naub0,9\\npci-2,9\\n2,9\\nnfn-2,9\\nauo0,9\\ncol- 0,9\\nRFI-O,9\\nAMp-2,9\\nabu-3,9\\nhlb-1,9\\nNbm-0,8\\ncol -0,8\\nrrt-0,8\\nasa -2,8\\nabi1-1,8\\nab-1,8\\nCol,8\\nrpt,8\\nANP-1,8\\ncwl-1,8\\nCWw-0,8\\nndb-1,8\\ncol-3,8\\nCWW-O,8\\nasc -3,7\\ncl-0,7\\nAMp-4,7\\nNPI-2,7\\npdl1o,7\\nASA -2,7\\npco-1,7\\nrtto,7\\nol-0,7\\n4/1/2014,7\\nCWL-1,7\\ncil-o,7\\nABo-2,7\\nASC-2,7\\nAB1-1,7\\nNBM-0-,7\\ncil -0,6\\nRVM-O,6\\npdp-1,6\\nCol Closed,6\\nhda-0,6\\nabi- 1,6\\nrrr-1,6\\nhto-0,6\\ncoL-0,6\\nnmo-3,6\\nasa-0,6\\npol-0,6\\nrao0,6\\nPrem+-1,6\\npfo-2,6\\nMTO,6\\nRGL,6\\nnpi-0,6\\nncl-2,6\\nncl2,6\\nABo-1,6\\ncoll-0,6\\nRTT-O,6\\nRPF-0,6\\nndb-0,6\\nMMO-0,5\\nPD1-2,5\\nnfn-0,5\\nAub-1,5\\nnfn-1,5\\nRTP-0,5\\nPDl-3,5\\nrrr--,5\\npdl-l,5\\nAU)-0,5\\nRRR-O,5\\naub-o,5\\npdL-1,5\\ncwl-o,5\\nRRR-0-,5\\nmao-0,5\\nNDB-1,5\\nPDV-0,5\\ncol--,5\\nrvm-o,5\\npolicy- ,5\\nRAA- 0,5\\nAB)-3,5\\nhlp-0,5\\nRao-0,4\\nAB0-1,4\\nHTU-0,4\\nrtp,4\\nRRR0-0,4\\nccp,4\\nnbE-1,4\\nCOL-O,4\\nnse-0,4\\nRWN,4\\n rtt-0,4\\nrtt-00,4\\nCRC-0,4\\nabi-i,4\\nABo-3,4\\nhdf-0,4\\nrai,4\\nraaa0,4\\n0-0,4\\nhxa-0,4\\nnbm-o,4\\nrtt0-0,4\\nPOLCIY,4\\nhca-0,4\\nMTO-0,4\\nrtt-o,4\\nrpe,4\\nrwn,4\\npolicy ,4\\nRDI-0,4\\nAMP-2170,4\\nab0-2,4\\ncil--0,4\\nrao-o,4\\nRRT-0,4\\nccp -0,3\\n`RRR-0,3\\nhch-0,3\\nhmp-1,3\\nab1-2,3\\nuab-02,3\\nHCH-0,3\\n-4,3\\nASC-3,3\\n COL-0,3\\nnbm-10,3\\nrrr-0 ,3\\nauu-6,3\\nrt-0,3\\nrto,3\\nCil-0,3\\nrvm-1,3\\nabu-2,3\\npem+-1,3\\nhdh,3\\nrow,2\\nABI -6,2\\nnsm-0,2\\nRRC-0,2\\nra0-0,2\\nRFI--0,2\\nNF,2\\neee-0,2\\nrtt-`0,2\\nNSM-3,2\\nrcvm-0,2\\nABi-5,2\\nrtp-0,2\\nhaw-0,2\\ncwl-c,2\\nrrr-`0,2\\nHDF-0,2\\nhcf-0,2\\nrwm-0,2\\nNSM-0,2\\nNSM-2,2\\npdlk-2,2\\nAB1-3,2\\nrpt0,2\\nhxf-0,2\\nnbe-100,2\\nrum-0,2\\nHDV-0,2\\nnem-4,2\\nRtt-0,2\\nroa-0,2\\nRWN -0,2\\nnem-3,2\\nnbbm-2,2\\nbbt-0,2\\npolicy-1,2\\nnmf-0,1\\nrpt--,1\\ncw-0,1\\nHXF-0,1\\ndpo-6,1\\nRRr-0,1\\nnbs-6,1\\nUNK,1\\nrr-0,1\\nnes-3,1\\nbcl-0,1\\nOTHER,1\\namo-2,1\\nnc-0,1\\nPREM+-0,1\\nrcl-0,1\\nHCF-0,1\\nhdp-0,1\\nNPI-1,1\\nrmn-0,1\\nCOL0,1\\nnbs-5,1\\n-abi,1\\ndpl-1,1\\nanf-6,1\\nhmo-0,1\\nhdu-0,1\\nzbi-1,1\\nauu-7,1\\nnoe-1,1\\nabi -4,1\\npdo-8,1\\n   ,1\\nnpp-3,1\\npsl-3,1\\ndo-1,1\\nauu-8,1\\nAUO-O,1\\ncwl-t,1\\ncwp-0,1\\npol-1,1\\nBI,1\\nani-1,1\\npdo-`1,1\\nauo-a,1\\nHCG-0,1\\nHCV-0,1\\nhcg-0,1\\nndm-0,1\\nASC,1\\nasi-3,1\\nndb-2,1\\nhdw-0,1\\n4,1\\nNBM-`1,1\\nNFN-1,1\\npfl-1,1\\nasc-0,1\\naub 0,1\\nnbv-1,1\\nayo-1,1\\nnbi-1,1\\nHTO-0,1\\nhtf-0,1\\nnoe-0,1\\nRLL-0,1\\nnbm-2637,1\\nCWL--0,1\\nRAI-0,1\\npdll-1,1\\ncrr-0,1\\nrf;-0,1\\ncel-0,1\\nRTTq-0,1\\nano-o,1\\n'\n",
      "s3fs OK\n"
     ]
    }
   ],
   "source": [
    "test_s3fs('metblue-working','symbolsicms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### awscli ###\n",
    "def test_awscli():\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.check_output(['aws','s3','ls'])\n",
    "        print('awscli OK')\n",
    "    except Exception as e:\n",
    "        print('awscli:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awscli OK\n"
     ]
    }
   ],
   "source": [
    "test_awscli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### boto3 ###\n",
    "def test_boto3():\n",
    "    try:\n",
    "        import boto3\n",
    "        s3 = boto3.resource('s3')\n",
    "        for bucket in s3.buckets.all():\n",
    "            print(bucket.name)\n",
    "        print('boto3 OK')\n",
    "    except Exception as e:\n",
    "        print('boto3:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-emr-resources-161266116677-us-east-1\n",
      "aws-logs-161266116677-us-east-1\n",
      "ayata\n",
      "ayata-database-backup\n",
      "ayata-server-p-keys\n",
      "ayata-software\n",
      "clr-data\n",
      "elasticbeanstalk-us-east-1-161266116677\n",
      "metblue-working\n",
      "metlife-claims-working\n",
      "metlife-digital-labor\n",
      "metlife-lead-allocation-working\n",
      "metlife-ultimate-loss\n",
      "oil-and-gas-resources\n",
      "boto3 OK\n"
     ]
    }
   ],
   "source": [
    "test_boto3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### seaborn ###\n",
    "def test_seaborn():\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        tips = sns.load_dataset(\"tips\")\n",
    "        g = sns.distplot(tips['total_bill'])\n",
    "        ax = g.get_figure()\n",
    "        ax.savefig('output.jpg')\n",
    "        ax.show()\n",
    "        print('seaborn OK')\n",
    "    except Exception as e:\n",
    "        print('seaborn:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atanu/anaconda3/lib/python3.6/site-packages/seaborn/distributions.py:218: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  color=hist_color, **hist_kws)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seaborn OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test_seaborn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### bokeh ###\n",
    "def test_bokeh():\n",
    "    try:\n",
    "        from bokeh.plotting import figure, output_file, show\n",
    "        p = figure()\n",
    "        p.circle([1, 2, 3], [4, 5, 6], color=\"orange\")\n",
    "        output_file(\"foo.html\")\n",
    "        show(p)\n",
    "        print('bokeh OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bokeh OK\n"
     ]
    }
   ],
   "source": [
    "test_bokeh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pydot ##\n",
    "def test_pydot():\n",
    "    try:\n",
    "        import pydot \n",
    "        graph = pydot.Dot(graph_type='graph')\n",
    "        for i in range(3):\n",
    "            edge = pydot.Edge(\"king\", \"lord%d\" % i)\n",
    "            graph.add_edge(edge)\n",
    "\n",
    "        vassal_num = 0\n",
    "        for i in range(3):\n",
    "            for j in range(2):\n",
    "                edge = pydot.Edge(\"lord%d\" % i, \"vassal%d\" % vassal_num)\n",
    "                graph.add_edge(edge)\n",
    "                vassal_num += 1\n",
    "\n",
    "        # Required: sudo apt install python-pydot python-pydot-ng graphviz\n",
    "        graph.write_png('example1_graph.png')\n",
    "        print('pydot OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydot OK\n"
     ]
    }
   ],
   "source": [
    "test_pydot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tensorflow ###\n",
    "def test_tensorflow():\n",
    "    try:\n",
    "        import tensorflow.compat.v1 as tf\n",
    "        tf.disable_v2_behavior() \n",
    "        import numpy\n",
    "        import matplotlib.pyplot as plt\n",
    "        rng = numpy.random\n",
    "        learning_rate = 0.01\n",
    "        training_epochs = 1000\n",
    "        display_step = 50\n",
    "        train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                                 7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "        train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                                 2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "        n_samples = train_X.shape[0]\n",
    "        X = tf.placeholder(\"float\")\n",
    "        Y = tf.placeholder(\"float\")\n",
    "        W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "        b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "        pred = tf.add(tf.multiply(X, W), b)\n",
    "        cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            for epoch in range(training_epochs):\n",
    "                for (x, y) in zip(train_X, train_Y):\n",
    "                    sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "                if (epoch+1) % display_step == 0:\n",
    "                    c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "                    print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                        \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "            print (\"Optimization Finished!\")\n",
    "            training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "            print (\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "            plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "            plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            print('tensorflow OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.337383211 W= -0.035147045 b= 2.8498704\n",
      "Epoch: 0100 cost= 0.307354718 W= -0.018212358 b= 2.728043\n",
      "Epoch: 0150 cost= 0.280790240 W= -0.0022845832 b= 2.6134603\n",
      "Epoch: 0200 cost= 0.257289827 W= 0.012696056 b= 2.5056903\n",
      "Epoch: 0250 cost= 0.236500129 W= 0.02678612 b= 2.404328\n",
      "Epoch: 0300 cost= 0.218110770 W= 0.04003643 b= 2.3090057\n",
      "Epoch: 0350 cost= 0.201841995 W= 0.05249923 b= 2.2193491\n",
      "Epoch: 0400 cost= 0.187449709 W= 0.0642209 b= 2.1350248\n",
      "Epoch: 0450 cost= 0.174716502 W= 0.07524615 b= 2.0557094\n",
      "Epoch: 0500 cost= 0.163451985 W= 0.08561578 b= 1.9811113\n",
      "Epoch: 0550 cost= 0.153486609 W= 0.09536863 b= 1.9109495\n",
      "Epoch: 0600 cost= 0.144671291 W= 0.10454075 b= 1.8449663\n",
      "Epoch: 0650 cost= 0.136872530 W= 0.11316738 b= 1.7829068\n",
      "Epoch: 0700 cost= 0.129972830 W= 0.12128129 b= 1.7245358\n",
      "Epoch: 0750 cost= 0.123868734 W= 0.12891269 b= 1.669636\n",
      "Epoch: 0800 cost= 0.118468255 W= 0.13609046 b= 1.6179997\n",
      "Epoch: 0850 cost= 0.113690481 W= 0.14284137 b= 1.5694339\n",
      "Epoch: 0900 cost= 0.109463856 W= 0.14919044 b= 1.5237598\n",
      "Epoch: 0950 cost= 0.105724722 W= 0.15516144 b= 1.4808048\n",
      "Epoch: 1000 cost= 0.102416433 W= 0.16077758 b= 1.4404025\n",
      "Optimization Finished!\n",
      "Training cost= 0.10241643 W= 0.16077758 b= 1.4404025 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow OK\n"
     ]
    }
   ],
   "source": [
    "test_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pytorch ##\n",
    "def test_pytorch():\n",
    "    try:\n",
    "        import torch\n",
    "        dtype = torch.float\n",
    "        device = torch.device(\"cpu\")\n",
    "        N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "        x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "        y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "        w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "        w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "        learning_rate = 1e-6\n",
    "        for t in range(500):\n",
    "            h = x.mm(w1)\n",
    "            h_relu = h.clamp(min=0)\n",
    "            y_pred = h_relu.mm(w2)\n",
    "            loss = (y_pred - y).pow(2).sum().item()\n",
    "            if t % 100 == 99:\n",
    "                print(t, loss)\n",
    "            grad_y_pred = 2.0 * (y_pred - y)\n",
    "            grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "            grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "            grad_h = grad_h_relu.clone()\n",
    "            grad_h[h < 0] = 0\n",
    "            grad_w1 = x.t().mm(grad_h)\n",
    "            w1 -= learning_rate * grad_w1\n",
    "            w2 -= learning_rate * grad_w2\n",
    "        print('pytorch OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 510.55755615234375\n",
      "199 2.6881985664367676\n",
      "299 0.024073349311947823\n",
      "399 0.00048731433344073594\n",
      "499 6.477403803728521e-05\n",
      "pytorch OK\n"
     ]
    }
   ],
   "source": [
    "test_pytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### keras ## \n",
    "def test_keras():\n",
    "    try:\n",
    "        from numpy import loadtxt\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense\n",
    "        import subprocess\n",
    "\n",
    "        subprocess.call(['wget','https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'])\n",
    "        dataset = loadtxt('pima-indians-diabetes.data.csv', delimiter=',')\n",
    "        # split into input (X) and output (y) variables\n",
    "        X = dataset[:,0:8]\n",
    "        y = dataset[:,8]\n",
    "        # define the keras model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # compile the keras model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        # fit the keras model on the dataset\n",
    "        model.fit(X, y, epochs=150, batch_size=10)\n",
    "        # evaluate the keras model\n",
    "        _, accuracy = model.evaluate(X, y)\n",
    "        print('Accuracy: %.2f' % (accuracy*100))\n",
    "        print('keras OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/atanu/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s 283us/step - loss: 27.9780 - accuracy: 0.3490\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 93us/step - loss: 2.7196 - accuracy: 0.4805\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.6913 - accuracy: 0.6393\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.6417 - accuracy: 0.6745\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.6337 - accuracy: 0.6836\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.6308 - accuracy: 0.6732\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.6298 - accuracy: 0.6771\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.6270 - accuracy: 0.6758\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.6209 - accuracy: 0.6784\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.6180 - accuracy: 0.6888\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 136us/step - loss: 0.6221 - accuracy: 0.6732\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 131us/step - loss: 0.6218 - accuracy: 0.6823\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.6134 - accuracy: 0.6862\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.6096 - accuracy: 0.6992\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.6135 - accuracy: 0.6862\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.6112 - accuracy: 0.7018\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.6134 - accuracy: 0.6836\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.6009 - accuracy: 0.7031\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.6055 - accuracy: 0.6966\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.6019 - accuracy: 0.7018\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5988 - accuracy: 0.7057\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5980 - accuracy: 0.7031\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.6043 - accuracy: 0.6914\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.6061 - accuracy: 0.6914\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 101us/step - loss: 0.5962 - accuracy: 0.6966\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5955 - accuracy: 0.7005\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5918 - accuracy: 0.7031\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5890 - accuracy: 0.7083\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5862 - accuracy: 0.7096\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5977 - accuracy: 0.6953\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5870 - accuracy: 0.7018\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.5751 - accuracy: 0.7135\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5848 - accuracy: 0.7057\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5824 - accuracy: 0.7096\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 107us/step - loss: 0.5814 - accuracy: 0.6966\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5730 - accuracy: 0.7161\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5738 - accuracy: 0.7018\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.5726 - accuracy: 0.7096\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.5712 - accuracy: 0.7044\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5713 - accuracy: 0.7083\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5741 - accuracy: 0.7161\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5699 - accuracy: 0.7070\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.5641 - accuracy: 0.7057\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.5679 - accuracy: 0.7122\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.5645 - accuracy: 0.7122\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.5681 - accuracy: 0.7057\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.5669 - accuracy: 0.6966\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.5673 - accuracy: 0.6992\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.5586 - accuracy: 0.7044\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5496 - accuracy: 0.7266\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5635 - accuracy: 0.7096\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5513 - accuracy: 0.7188\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5530 - accuracy: 0.7096\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5541 - accuracy: 0.7305\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5469 - accuracy: 0.7227\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 138us/step - loss: 0.5484 - accuracy: 0.7266\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 147us/step - loss: 0.5596 - accuracy: 0.7174\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 158us/step - loss: 0.5521 - accuracy: 0.7253\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.5457 - accuracy: 0.7331\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5467 - accuracy: 0.7253\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5594 - accuracy: 0.6979\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5443 - accuracy: 0.7292\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.5451 - accuracy: 0.7201\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.5507 - accuracy: 0.7148\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5440 - accuracy: 0.7227\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5375 - accuracy: 0.7279\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.5418 - accuracy: 0.7331\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5404 - accuracy: 0.7305\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5374 - accuracy: 0.7435\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5392 - accuracy: 0.7357\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5385 - accuracy: 0.7266\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5457 - accuracy: 0.7214\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.5403 - accuracy: 0.7266\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5313 - accuracy: 0.7279\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.5356 - accuracy: 0.7435\n",
      "Epoch 76/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 107us/step - loss: 0.5359 - accuracy: 0.7318\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 146us/step - loss: 0.5329 - accuracy: 0.7357\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5342 - accuracy: 0.7500\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.5421 - accuracy: 0.7435\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5332 - accuracy: 0.7383\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 127us/step - loss: 0.5337 - accuracy: 0.7344\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 124us/step - loss: 0.5244 - accuracy: 0.7422\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 123us/step - loss: 0.5272 - accuracy: 0.7370\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5264 - accuracy: 0.7383\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.5391 - accuracy: 0.7279\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5273 - accuracy: 0.7422\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.5312 - accuracy: 0.7383\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5237 - accuracy: 0.7409\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5231 - accuracy: 0.7409\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5261 - accuracy: 0.7448\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5318 - accuracy: 0.7357\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5348 - accuracy: 0.7318\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5251 - accuracy: 0.7331\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5273 - accuracy: 0.7344\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5189 - accuracy: 0.7500\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.5226 - accuracy: 0.7357\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 116us/step - loss: 0.5169 - accuracy: 0.7539\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 129us/step - loss: 0.5283 - accuracy: 0.7370\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.5233 - accuracy: 0.7370\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.5154 - accuracy: 0.7474\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 145us/step - loss: 0.5222 - accuracy: 0.7396\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5222 - accuracy: 0.7448\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5219 - accuracy: 0.7383\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 138us/step - loss: 0.5224 - accuracy: 0.7383\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 128us/step - loss: 0.5172 - accuracy: 0.7552\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.5265 - accuracy: 0.7435\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5127 - accuracy: 0.7422\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5155 - accuracy: 0.7461\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5180 - accuracy: 0.7539\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5164 - accuracy: 0.7539\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5132 - accuracy: 0.7435\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5099 - accuracy: 0.7500\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5135 - accuracy: 0.7539\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5078 - accuracy: 0.7474\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.5124 - accuracy: 0.7422\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 125us/step - loss: 0.5069 - accuracy: 0.7578\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 110us/step - loss: 0.5172 - accuracy: 0.7500\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.5081 - accuracy: 0.7474\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 126us/step - loss: 0.5170 - accuracy: 0.7357\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.5067 - accuracy: 0.7513\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5104 - accuracy: 0.7487\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5072 - accuracy: 0.7526\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 102us/step - loss: 0.5106 - accuracy: 0.7500\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5090 - accuracy: 0.7526\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 216us/step - loss: 0.5083 - accuracy: 0.7591\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 221us/step - loss: 0.5129 - accuracy: 0.7526\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.5134 - accuracy: 0.7500\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4980 - accuracy: 0.7617\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 119us/step - loss: 0.5105 - accuracy: 0.7591\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5068 - accuracy: 0.7487\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4994 - accuracy: 0.7591\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 105us/step - loss: 0.5143 - accuracy: 0.7630\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 108us/step - loss: 0.5038 - accuracy: 0.7513\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 135us/step - loss: 0.5031 - accuracy: 0.7500\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5058 - accuracy: 0.7526\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 112us/step - loss: 0.5004 - accuracy: 0.7565\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 117us/step - loss: 0.5002 - accuracy: 0.7708\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 145us/step - loss: 0.5142 - accuracy: 0.7461\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.5024 - accuracy: 0.7539\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 104us/step - loss: 0.5023 - accuracy: 0.7656\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 121us/step - loss: 0.4976 - accuracy: 0.7500\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.4972 - accuracy: 0.7513\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 111us/step - loss: 0.4936 - accuracy: 0.7617\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 103us/step - loss: 0.4952 - accuracy: 0.7682\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 113us/step - loss: 0.4990 - accuracy: 0.7487\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 109us/step - loss: 0.4927 - accuracy: 0.7565\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 106us/step - loss: 0.5017 - accuracy: 0.7565\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.4950 - accuracy: 0.7578\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 115us/step - loss: 0.4879 - accuracy: 0.7695\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 114us/step - loss: 0.5027 - accuracy: 0.7513\n",
      "768/768 [==============================] - 0s 49us/step\n",
      "Accuracy: 76.43\n",
      "keras OK\n"
     ]
    }
   ],
   "source": [
    "test_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### theano ##\n",
    "def test_theano():\n",
    "    try:\n",
    "        import numpy \n",
    "        import theano.tensor as T \n",
    "        from theano import function \n",
    "        x = T.dscalar('x') \n",
    "        y = T.dscalar('y') \n",
    "        z = x + y \n",
    "        f = function([x, y], z) \n",
    "        print(f(5, 7))\n",
    "        print('theano OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "theano OK\n"
     ]
    }
   ],
   "source": [
    "test_theano()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lightgbm ##\n",
    "def test_lightgbm():\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import lightgbm as lgb\n",
    "        from sklearn import preprocessing\n",
    "        import subprocess\n",
    "        subprocess.call(['wget','https://raw.githubusercontent.com/serengil/decision-trees-for-ml/master/dataset/golf2.txt'])\n",
    "        dataset = pd.read_csv('golf2.txt')\n",
    "        target_name = 'Decision'\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        features = []; categorical_features = []\n",
    "        num_of_columns = dataset.shape[1]\n",
    "        for i in range(0, num_of_columns):\n",
    "            column_name = dataset.columns[i]\n",
    "            column_type = dataset[column_name].dtypes\n",
    "            if i != num_of_columns - 1: #skip target\n",
    "                features.append(column_name)\n",
    "            if column_type == 'object':\n",
    "                le.fit(dataset[column_name])\n",
    "                feature_classes = list(le.classes_)\n",
    "                encoded_feature = le.transform(dataset[column_name])\n",
    "                dataset[column_name] = pd.DataFrame(encoded_feature)\n",
    "            if i != num_of_columns - 1: #skip target\n",
    "                categorical_features.append(column_name)\n",
    "        num_of_classes = len(feature_classes)\n",
    "        y_train = dataset['Decision'].values\n",
    "        x_train = dataset.drop(columns=['Decision']).values\n",
    "        lgb_train = lgb.Dataset(x_train, y_train\n",
    "         ,feature_name = features\n",
    "         , categorical_feature = categorical_features\n",
    "        )\n",
    "        params = {\n",
    "         'task': 'train'\n",
    "         , 'boosting_type': 'gbdt'\n",
    "         , 'objective': 'multiclass'\n",
    "         , 'num_class': num_of_classes\n",
    "         , 'metric': 'multi_logloss'\n",
    "         , 'min_data': 1\n",
    "         , 'verbose': -1\n",
    "        }\n",
    "        gbm = lgb.train(params, lgb_train, num_boost_round=50)\n",
    "        predictions = gbm.predict(x_train)\n",
    "        for index, instance in dataset.iterrows():\n",
    "            actual = instance[target_name]\n",
    "            prediction = np.argmax(predictions[index])\n",
    "            print((index+1), 'actual=',actual, 'prediction=',prediction)\n",
    "        print('lightgbm OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 actual= 0 prediction= 0\n",
      "2 actual= 0 prediction= 0\n",
      "3 actual= 1 prediction= 1\n",
      "4 actual= 1 prediction= 1\n",
      "5 actual= 1 prediction= 1\n",
      "6 actual= 0 prediction= 0\n",
      "7 actual= 1 prediction= 1\n",
      "8 actual= 0 prediction= 0\n",
      "9 actual= 1 prediction= 1\n",
      "10 actual= 1 prediction= 1\n",
      "11 actual= 1 prediction= 1\n",
      "12 actual= 1 prediction= 1\n",
      "13 actual= 1 prediction= 1\n",
      "14 actual= 0 prediction= 0\n",
      "lightgbm OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atanu/.local/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    }
   ],
   "source": [
    "test_lightgbm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Catboost ##\n",
    "def test_catboost():\n",
    "    try:\n",
    "        from catboost import CatBoostRegressor\n",
    "        train_data = [[1, 4, 5, 6],\n",
    "                      [4, 5, 6, 7],\n",
    "                      [30, 40, 50, 60]]\n",
    "        eval_data = [[2, 4, 6, 8],\n",
    "                     [1, 4, 50, 60]]\n",
    "        train_labels = [10, 20, 30]\n",
    "        # Initialize CatBoostRegressor\n",
    "        model = CatBoostRegressor(iterations=2,\n",
    "                                  learning_rate=1,\n",
    "                                  depth=2)\n",
    "        # Fit model\n",
    "        model.fit(train_data, train_labels)\n",
    "        # Get predictions\n",
    "        preds = model.predict(eval_data)\n",
    "        print('catboost OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 6.4355782\ttotal: 48.6ms\tremaining: 48.6ms\n",
      "1:\tlearn: 4.8266836\ttotal: 50ms\tremaining: 0us\n",
      "catboost OK\n"
     ]
    }
   ],
   "source": [
    "test_catboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### XGBoost ##\n",
    "def test_xgboost():\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.datasets import load_boston\n",
    "        boston = load_boston()\n",
    "        data = pd.DataFrame(boston.data)\n",
    "        X, y = data.iloc[:,:-1],data.iloc[:,-1]\n",
    "        data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "        xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                        max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "        xg_reg.fit(X_train,y_train)\n",
    "        preds = xg_reg.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        print(\"RMSE: %f\" % (rmse))\n",
    "        print('xgboost OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:16] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE: 6.286391\n",
      "xgboost OK\n"
     ]
    }
   ],
   "source": [
    "test_xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pyflux ##\n",
    "def test_pyflux():\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import pyflux as pf\n",
    "        from datetime import datetime\n",
    "        data = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/drivers.csv\")\n",
    "        data.index = data['time'];\n",
    "        data.loc[(data['time']>=1983.05), 'seat_belt'] = 1;\n",
    "        data.loc[(data['time']<1983.05), 'seat_belt'] = 0;\n",
    "        data.loc[(data['time']>=1974.00), 'oil_crisis'] = 1;\n",
    "        data.loc[(data['time']<1974.00), 'oil_crisis'] = 0;\n",
    "        data['drivers'] = data['value']\n",
    "        model = pf.ARIMAX(data=data, formula='drivers~1+seat_belt+oil_crisis',\n",
    "                          ar=1, ma=1, family=pf.Normal())\n",
    "        x = model.fit(\"MLE\")\n",
    "        x.summary()\n",
    "        print('pyflux OK')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal ARIMAX(1,0,1)                                                                                      \n",
      "======================================================= ==================================================\n",
      "Dependent Variable: drivers                             Method: MLE                                       \n",
      "Start Date: 1969.08333333333                            Log Likelihood: -1278.7644                        \n",
      "End Date: 1984.91666666667                              AIC: 2569.5288                                    \n",
      "Number of observations: 191                             BIC: 2589.0424                                    \n",
      "==========================================================================================================\n",
      "Latent Variable                          Estimate   Std Error  z        P>|z|    95% C.I.                 \n",
      "======================================== ========== ========== ======== ======== =========================\n",
      "AR(1)                                    0.5004     0.0933     5.3637   0.0      (0.3175 | 0.6832)        \n",
      "MA(1)                                    0.1711     0.0991     1.7258   0.0844   (-0.0232 | 0.3654)       \n",
      "Beta 1                                   946.2139   176.9077   5.3486   0.0      (599.4748 | 1292.953)    \n",
      "Beta seat_belt                           -57.7772   57.817     -0.9993  0.3176   (-171.0986 | 55.5442)    \n",
      "Beta oil_crisis                          -151.6121  44.1128    -3.4369  0.0006   (-238.0731 | -65.1511)   \n",
      "Normal Scale                             195.6169                                                         \n",
      "==========================================================================================================\n",
      "pyflux OK\n"
     ]
    }
   ],
   "source": [
    "test_pyflux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
